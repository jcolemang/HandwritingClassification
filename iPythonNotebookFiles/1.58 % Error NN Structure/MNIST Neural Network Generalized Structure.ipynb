{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for MNIST Problem\n",
    "\n",
    "The following document contains generalized code to create a fully connected network to solve the MNIST Problem.\n",
    "\n",
    "### Tunable Features:\n",
    "\n",
    "Any number of hidden layers\n",
    "\n",
    "Any size for the hidden layers\n",
    "\n",
    "Any function for hidden layer/output layer (sigmoid, softmax, tanh, and ReLU are given)\n",
    "\n",
    "Minibatch gradient descent\n",
    "\n",
    "Learning rate (for both biases and synapses)\n",
    "\n",
    "Momentum\n",
    "\n",
    "Max-norm regularization\n",
    "\n",
    "Dropout on input and hidden layers\n",
    "\n",
    "Synapse size (Synapses are instantiated to random values in [-1,1])\n",
    "\n",
    "\n",
    "### Sources \n",
    "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf - Tuning hyperparameters and structure of the final network, dropout tips\n",
    "\n",
    "https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts - 10 nodes as output rather than 1\n",
    "\n",
    "http://iamtrask.github.io/2015/07/28/dropout/ - Implementing dropout to NN\n",
    "\n",
    "http://iamtrask.github.io/2015/07/12/basic-python-network/ - Original struture of the network\n",
    "\n",
    "https://en.wikipedia.org/ Regarding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Setup Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Retrieve the test data from the filesystem\n",
    "data = pd.read_csv(\"Kaggle Competition MINST train.csv\")\n",
    "labels = data['label']\n",
    "data = data.drop('label', axis=1)\n",
    "data = data.div(255)\n",
    "\n",
    "# Retrieve labels for the test data\n",
    "target = pd.read_csv(\"Kaggle Competition MINST train revised target.csv\")\n",
    "target = target.drop(\"Unnamed: 0\", axis = 1)\n",
    "\n",
    "# Split the training data so that I can analyze testing error\n",
    "train_data, test_data, train_target, test_target = cross_validation.train_test_split(\n",
    " data, target, test_size=0.25, random_state=0)\n",
    "\n",
    "# Get test data in label form\n",
    "test_data_labels = np.zeros(len(test_target), dtype=np.int8)\n",
    "test_target_arr = np.array(test_target)\n",
    "for i in range(len(test_target)):\n",
    "    test_data_labels[i] = test_target_arr[i].argmax()\n",
    "\n",
    "# Get the train data in label form\n",
    "train_data_labels = np.zeros(len(train_target), dtype=np.int8)\n",
    "train_target_arr = np.array(train_target)\n",
    "for i in range(len(train_target)):\n",
    "    train_data_labels[i] = train_target_arr[i].argmax()    \n",
    "    \n",
    "num_attributes = len(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rectifier function\n",
    "def rectifier(x, deriv=False):\n",
    "    flag = x > 0\n",
    "    if(deriv==True):\n",
    "        return 1 if flag else 0  #1 / (1 + np.exp(-x))\n",
    "    return x if flag else 0  #np.log(1 + np.exp(x))\n",
    "rectifier = np.vectorize(rectifier)\n",
    "\n",
    "# softmax function\n",
    "def softmax(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1\n",
    "    exp_scores = np.exp(x)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# tanh function\n",
    "def tanh(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1 - np.power(x,2)\n",
    "    return np.tanh(x)\n",
    "\n",
    "# sigmoid/logistic function\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural network architecture parameters\n",
    "hidden_layer_funct = sigmoid\n",
    "output_layer_funct = softmax\n",
    "\n",
    "input_layer_size = num_attributes\n",
    "num_hidden_layers = 3\n",
    "hidden_layer_size = 1024\n",
    "output_layer_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for learning\n",
    "mini_batch_size = 100\n",
    "training_steps = 10\n",
    "epsilon = 0.0004\n",
    "momentum = 0.90\n",
    "synapse_size = 1\n",
    "bias_size = 1\n",
    "max_norm = True\n",
    "c = 10\n",
    "hidden_dropout, input_dropout, do_dropout = (0.5, 0.80, True)\n",
    "if do_dropout:\n",
    "    epsilon *= 7\n",
    "if do_dropout:\n",
    "    momentum = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Synapse/bias instantiation\n",
    "synapse = []\n",
    "synapse_deltas = []\n",
    "biases = []\n",
    "bias_deltas = []\n",
    "prev_test_error = 1.0\n",
    "\n",
    "# Input-Hidden synapse\n",
    "if(hidden_layer_funct == sigmoid or hidden_layer_funct == tanh):\n",
    "    synapse.append(2 * np.random.random((input_layer_size, hidden_layer_size)) - 1)\n",
    "if(hidden_layer_funct == rectifier):\n",
    "    synapse.append(np.ones((input_layer_size, hidden_layer_size)))\n",
    "synapse_deltas.append(np.zeros((input_layer_size, hidden_layer_size)))\n",
    "biases.append(np.zeros(hidden_layer_size))\n",
    "bias_deltas.append(np.zeros(hidden_layer_size))\n",
    "\n",
    "# Hidden-Hidden synapse\n",
    "for layer in range(1, num_hidden_layers):\n",
    "    if(hidden_layer_funct == sigmoid or hidden_layer_funct == tanh):\n",
    "        synapse.append(2 * np.random.random((hidden_layer_size, hidden_layer_size)) - 1)\n",
    "    if(hidden_layer_funct == rectifier):\n",
    "        synapse.append(np.ones((hidden_layer_size, hidden_layer_size)))\n",
    "    synapse_deltas.append(np.zeros((hidden_layer_size, hidden_layer_size)))\n",
    "    biases.append(np.zeros(hidden_layer_size))\n",
    "    bias_deltas.append(np.zeros(hidden_layer_size))\n",
    "\n",
    "# Hidden-Output synapse\n",
    "synapse.append(2 * np.random.random((hidden_layer_size, output_layer_size)) - 1)\n",
    "synapse_deltas.append(np.zeros((hidden_layer_size, output_layer_size)))\n",
    "\n",
    "synapse = map(lambda(x): x*synapse_size, synapse)\n",
    "\n",
    "def pred(input):\n",
    "    current_layer = input\n",
    "    for layer in range(num_hidden_layers):\n",
    "        current_layer = hidden_layer_funct(np.dot(current_layer, synapse[layer])\n",
    "                                            + biases[layer])\n",
    "        \n",
    "    output = output_layer_funct(np.dot(current_layer, synapse[num_hidden_layers]) + biases[num_hidden_layers])\n",
    "    \n",
    "    answer = np.zeros(len(output), dtype = np.int8)\n",
    "    for i in range(len(output)):\n",
    "        answer[i] = output[i].argmax()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration:  499\n",
      "Training Error:  0.0\n",
      "Testing Error:  0.0158095238095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple training runs Stoichiastic training of the neural network\n",
    "for iteration in xrange(training_steps):\n",
    "    \n",
    "    # Create a set of minibatches for training\n",
    "    all_inds = np.array(train_data.index)\n",
    "    random.shuffle(all_inds)\n",
    "    batch_inds = [all_inds[i:i+mini_batch_size] for i in xrange(0,\n",
    "                                                len(all_inds), mini_batch_size)]\n",
    "\n",
    "    # train on each batch\n",
    "    for inds in batch_inds:\n",
    "        X = np.array(train_data.ix[inds])\n",
    "        y = np.array(train_target.ix[inds])\n",
    "\n",
    "        layers = []\n",
    "        layer_deltas = []\n",
    "        # Input layer\n",
    "        layers.append(X)\n",
    "        if(do_dropout):\n",
    "            layers[0] *= np.random.binomial([np.ones(layers[0].shape)],\n",
    "                          input_dropout)[0]\n",
    "            \n",
    "        # Hidden Layers\n",
    "        for layer in range(num_hidden_layers):\n",
    "            layers.append(hidden_layer_funct(np.dot(layers[layer], synapse[layer])\n",
    "                                                                  + biases[layer]))\n",
    "            layer_deltas.append(np.zeros((hidden_layer_size, mini_batch_size)))\n",
    "            if(do_dropout):\n",
    "                layers[layer + 1] *= np.random.binomial([np.ones(layers[layer+1].shape)],\n",
    "                                      hidden_dropout)[0]     \n",
    "                \n",
    "        # Output Layer\n",
    "        layers.append(output_layer_funct(np.dot(layers[num_hidden_layers], synapse[num_hidden_layers])))\n",
    "        layer_deltas.append(np.zeros((output_layer_size, mini_batch_size)))\n",
    "        \n",
    "        # Backpropogation: Layer contribution to errors\n",
    "        layer_deltas[-1] = (layers[-1] - y) * output_layer_funct(layers[-1], True)\n",
    "        for layer in range(2, num_hidden_layers + 2):\n",
    "            layer_deltas[-layer] = layer_deltas[-(layer - 1)].dot(\n",
    "                                    synapse[-(layer - 1)].T) * hidden_layer_funct(\n",
    "                                    layers[-layer], True)\n",
    "            \n",
    "        # Update synapses\n",
    "        for index in range(len(synapse)):\n",
    "            synapse_deltas[index] = layers[index].T.dot(\n",
    "                                     layer_deltas[index]) * epsilon + momentum*synapse_deltas[index]\n",
    "            synapse[index] -= synapse_deltas[index]\n",
    "            \n",
    "        # Update biases\n",
    "        for index in range(len(biases)):\n",
    "            bias_deltas[index] = np.ones(mini_batch_size).dot(\n",
    "                                  layer_deltas[index]) * epsilon + momentum*bias_deltas[index]\n",
    "            biases[index] -= bias_deltas[index]\n",
    "        \n",
    "        # Find magnitude of neuron vectors and preform max-normalization\n",
    "        if(max_norm):\n",
    "            for i in range(len(synapse)):\n",
    "                syn_mag = np.linalg.norm(synapse[i], axis = 0)\n",
    "                if(any(syn_mag > c)):\n",
    "                    synapse[i] = synapse[i] / (syn_mag / c)\n",
    "\n",
    "# Scale down the synapses to accomadate using the network without dropout\n",
    "if do_dropout:\n",
    "    synapse[0] *= input_dropout\n",
    "    for i in range(1, len(synapse)):\n",
    "        synapse[i] *= hidden_dropout\n",
    "        \n",
    "# Add a set of zeros for the out layer bias\n",
    "biases.append(np.zeros(output_layer_size))\n",
    "        \n",
    "# Test the model at this iteration\n",
    "prediction = pred(train_data)\n",
    "train_error = 1 - accuracy_score(train_data_labels, prediction)\n",
    "prediction = pred(test_data)\n",
    "test_error = 1 - accuracy_score(test_data_labels, prediction)\n",
    "\n",
    "print \"Training Iteration: \" , iteration\n",
    "print \"Training Error: \", train_error\n",
    "print \"Testing Error: \" , test_error\n",
    "print\n",
    "\n",
    "prev_test_error = test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "0.0006 - 0.0008\n",
    "0.0005 - 0.00019\n",
    "0.0004 - 0.00009\n",
    "0.0001 - 0.01\n",
    "\n",
    "*20 - 0.8\n",
    "*10 - 0.10\n",
    "*8 - 0.0383\n",
    "*7 - 0.03785\n",
    "*6 - 0.038\n",
    "*5 - 0.0409\n",
    "*4 - 0.0459\n",
    "*3 - 0.0507\n",
    "*1 - 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37166659, -0.57607966, -0.83539794, ..., -0.72201505,\n",
       "       -0.60238743, -0.06176436])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Network Synapses and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the synapses for later use\n",
    "for index in range(len(synapse)):\n",
    "    snaypse_i = pd.DataFrame(synapse[index])\n",
    "    bias_i = pd.DataFrame(biases[index])\n",
    "    snaypse_i.to_csv(\"%d-Layer %s-%s %d-%d-%d nodes syn%d.csv\" % (\n",
    "            num_hidden_layers, hidden_layer_funct.func_name, output_layer_funct.func_name, input_layer_size, hidden_layer_size, output_layer_size, index))\n",
    "    bias_i.to_csv(\"%d-Layer %s-%s %d-%d-%d nodes bias%d.csv\" % (\n",
    "            num_hidden_layers, hidden_layer_funct.func_name, output_layer_funct.func_name, input_layer_size, hidden_layer_size, output_layer_size, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Retrieve the test data from the filesystem\n",
    "data = pd.read_csv(\"Kaggle Competition MINST train.csv\")\n",
    "target = data['label']\n",
    "data = data.drop('label', axis=1)\n",
    "data = data.div(255)\n",
    "\n",
    "# Split the training data so that I can analyze testing error (same split as training)\n",
    "train_data, test_data, train_target, test_target = cross_validation.train_test_split(\n",
    " data, target, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural network architecture parameters\n",
    "hidden_layer_funct = sigmoid\n",
    "output_layer_funct = softmax\n",
    "\n",
    "input_layer_size = num_attributes\n",
    "num_hidden_layers = 3\n",
    "hidden_layer_size = 1024\n",
    "output_layer_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the synapses for later use\n",
    "synapse = []\n",
    "biases = []\n",
    "for index in range(num_hidden_layers + 1):\n",
    "    syn_filename = \"%d-Layer %s-%s %d-%d-%d nodes syn%d.csv\" % (\n",
    "       num_hidden_layers, hidden_layer_funct.func_name, output_layer_funct.func_name, input_layer_size, hidden_layer_size, output_layer_size, index)\n",
    "    bias_filename = \"%d-Layer %s-%s %d-%d-%d nodes bias%d.csv\" % (\n",
    "       num_hidden_layers, hidden_layer_funct.func_name, output_layer_funct.func_name, input_layer_size, hidden_layer_size, output_layer_size, index)\n",
    "    synapse.append(np.array(pd.read_csv(syn_filename).drop(\"Unnamed: 0\", axis = 1)))\n",
    "    biases.append(np.array(pd.read_csv(bias_filename).drop(\"Unnamed: 0\", axis = 1)))\n",
    "\n",
    "# rectifier function\n",
    "def rectifier(x, deriv=False):\n",
    "    flag = x > 0\n",
    "    if(deriv==True):\n",
    "        return 1 if flag else 0  #1 / (1 + np.exp(-x))\n",
    "    return x if flag else 0  #np.log(1 + np.exp(x))\n",
    "rectifier = np.vectorize(rectifier)\n",
    "\n",
    "# softmax function\n",
    "def softmax(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1\n",
    "    exp_scores = np.exp(x)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# tanh function\n",
    "def tanh(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1 - np.power(x,2)\n",
    "    return np.tanh(x)\n",
    "\n",
    "# sigmoid/logistic function\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def pred(input):\n",
    "    current_layer = input\n",
    "    for layer in range(num_hidden_layers):\n",
    "        current_layer = hidden_layer_funct(np.dot(current_layer, synapse[layer])\n",
    "                                            + biases[layer].T)\n",
    "        \n",
    "    output = output_layer_funct(np.dot(current_layer, synapse[num_hidden_layers]) + biases[num_hidden_layers].T)\n",
    "    \n",
    "    answer = np.zeros(len(output), dtype = np.int8)\n",
    "    for i in range(len(output)):\n",
    "        answer[i] = output[i].argmax()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration:  499\n",
      "Training Error:  0.0\n",
      "Testing Error:  0.0158095238095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model at this iteration\n",
    "prediction = pred(train_data)\n",
    "train_error = 1 - accuracy_score(train_data_labels, prediction)\n",
    "prediction = pred(test_data)\n",
    "test_error = 1 - accuracy_score(test_data_labels, prediction)\n",
    "\n",
    "print \"Training Error: \", train_error\n",
    "print \"Testing Error: \" , test_error\n",
    "print\n",
    "\n",
    "prev_test_error = test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
